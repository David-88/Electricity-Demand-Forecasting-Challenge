{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "import tqdm\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data is:\n",
      "           id  dt  type  target\n",
      "0  00037f39cf  11     2  44.050\n",
      "1  00037f39cf  12     2  50.672\n",
      "2  00037f39cf  13     2  39.042\n",
      "3  00037f39cf  14     2  35.900\n",
      "4  00037f39cf  15     2  53.888\n",
      "Test data is:\n",
      "           id  dt  type\n",
      "0  00037f39cf   1     2\n",
      "1  00037f39cf   2     2\n",
      "2  00037f39cf   3     2\n",
      "3  00037f39cf   4     2\n",
      "4  00037f39cf   5     2\n"
     ]
    }
   ],
   "source": [
    "# 2）探索性数据分析（EDA）\n",
    "# 在数据准备阶段，主要读取训练数据和测试数据，并进行基本的数据展示。\n",
    "# 加载数据\n",
    "train = pd.read_csv('data/dataset/train.csv')\n",
    "print(\"Train data is:\")\n",
    "print(train.head())\n",
    "test = pd.read_csv('data/dataset/test.csv')\n",
    "print(\"Test data is:\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并训练数据和测试数据\n",
    "# 首先，将训练数据 (train) 和测试数据 (test) 合并成一个整体数据框 data——方便排序和平移\n",
    "data = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "# 然后根据 id 和 dt（时间）进行降序排序，确保每个 id 的数据按时间从近到远排列。 ascending=Fasle 倒序\n",
    "data = data.sort_values(['id','dt'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面正式进行特征工程的步骤\n",
    "\n",
    "# 历史平移\n",
    "# 这里进行了目标值 (target) 的平移操作，生成 target_shift{i} 特征。\n",
    "# 通过 groupby 和 shift 操作，对每个 id，将目标值向后平移 i 步（从 10 到 35）。\n",
    "# 例如，target_shift10 表示该行目标值是10步之前的目标值——前面按照倒序排过了\n",
    "for i in range(10,36):\n",
    "    data[f'target_shift{i}'] = data.groupby('id')['target'].shift(i)\n",
    "\n",
    "# 历史平移 + 差分特征\n",
    "# 对于 target_shift10 特征，生成其差分特征。\n",
    "# 通过 diff 操作，计算平移后的目标值在不同步数下的差异。\n",
    "# 例如，target_shift10_diff1 表示 target_shift10 和前一个时间步的差值。\n",
    "for i in range(1,4):\n",
    "    data[f'target_shift10_diff{i}'] = data.groupby('id')['target_shift10'].diff(i)\n",
    "    \n",
    "# 窗口统计\n",
    "# 对目标值进行滑动窗口统计，生成均值、最大值、最小值和标准差特征。\n",
    "# 使用 rolling 函数，对每个 id 进行滑动窗口操作，窗口大小分别为 15、30、50 和 70。\n",
    "# min_periods=3 表示窗口内至少需要3个非缺失值才能计算统计量。\n",
    "for win in [15,30,50,70]:\n",
    "    data[f'target_win{win}_mean'] = data.groupby('id')['target'].rolling(window=win, min_periods=3, closed='left').mean().values\n",
    "    data[f'target_win{win}_max'] = data.groupby('id')['target'].rolling(window=win, min_periods=3, closed='left').max().values\n",
    "    data[f'target_win{win}_min'] = data.groupby('id')['target'].rolling(window=win, min_periods=3, closed='left').min().values\n",
    "    data[f'target_win{win}_std'] = data.groupby('id')['target'].rolling(window=win, min_periods=3, closed='left').std().values\n",
    "\n",
    "# 历史平移 + 窗口统计\n",
    "# 对 target_shift10 特征进行滑动窗口统计，生成均值、最大值、最小值、和标准差特征。\n",
    "# 使用 rolling 函数，窗口大小分别为 7、14、28、35、50 和 70。\n",
    "# 这些特征结合了历史平移和平滑窗口的概念，更详细地捕捉时间序列中的变化。\n",
    "for win in [7,14,28,35,50,70]:\n",
    "    data[f'target_shift10_win{win}_mean'] = data.groupby('id')['target_shift10'].rolling(window=win, min_periods=3, closed='left').mean().values\n",
    "    data[f'target_shift10_win{win}_max'] = data.groupby('id')['target_shift10'].rolling(window=win, min_periods=3, closed='left').max().values\n",
    "    data[f'target_shift10_win{win}_min'] = data.groupby('id')['target_shift10'].rolling(window=win, min_periods=3, closed='left').min().values\n",
    "    data[f'target_shift10_win{win}_sum'] = data.groupby('id')['target_shift10'].rolling(window=win, min_periods=3, closed='left').sum().values\n",
    "    data[f'target_shift710win{win}_std'] = data.groupby('id')['target_shift10'].rolling(window=win, min_periods=3, closed='left').std().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train前5行数据\n",
      "           id   dt  type  target  target_shift10  target_shift11  \\\n",
      "0  fff81139a7  506     5  18.145             NaN             NaN   \n",
      "1  fff81139a7  505     5  22.021             NaN             NaN   \n",
      "2  fff81139a7  504     5  21.282             NaN             NaN   \n",
      "3  fff81139a7  503     5  22.818             NaN             NaN   \n",
      "4  fff81139a7  502     5  28.552             NaN             NaN   \n",
      "\n",
      "   target_shift12  target_shift13  target_shift14  target_shift15  ...  \\\n",
      "0             NaN             NaN             NaN             NaN  ...   \n",
      "1             NaN             NaN             NaN             NaN  ...   \n",
      "2             NaN             NaN             NaN             NaN  ...   \n",
      "3             NaN             NaN             NaN             NaN  ...   \n",
      "4             NaN             NaN             NaN             NaN  ...   \n",
      "\n",
      "   target_shift10_win50_mean  target_shift10_win50_max  \\\n",
      "0                        NaN                       NaN   \n",
      "1                        NaN                       NaN   \n",
      "2                        NaN                       NaN   \n",
      "3                        NaN                       NaN   \n",
      "4                        NaN                       NaN   \n",
      "\n",
      "   target_shift10_win50_min  target_shift10_win50_sum  \\\n",
      "0                       NaN                       NaN   \n",
      "1                       NaN                       NaN   \n",
      "2                       NaN                       NaN   \n",
      "3                       NaN                       NaN   \n",
      "4                       NaN                       NaN   \n",
      "\n",
      "   target_shift710win50_std  target_shift10_win70_mean  \\\n",
      "0                       NaN                        NaN   \n",
      "1                       NaN                        NaN   \n",
      "2                       NaN                        NaN   \n",
      "3                       NaN                        NaN   \n",
      "4                       NaN                        NaN   \n",
      "\n",
      "   target_shift10_win70_max  target_shift10_win70_min  \\\n",
      "0                       NaN                       NaN   \n",
      "1                       NaN                       NaN   \n",
      "2                       NaN                       NaN   \n",
      "3                       NaN                       NaN   \n",
      "4                       NaN                       NaN   \n",
      "\n",
      "   target_shift10_win70_sum  target_shift710win70_std  \n",
      "0                       NaN                       NaN  \n",
      "1                       NaN                       NaN  \n",
      "2                       NaN                       NaN  \n",
      "3                       NaN                       NaN  \n",
      "4                       NaN                       NaN  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "test前5行数据\n",
      "           id  dt  type  target  target_shift10  target_shift11  \\\n",
      "0  fff81139a7  10     5     NaN          29.571          33.691   \n",
      "1  fff81139a7   9     5     NaN          28.677          29.571   \n",
      "2  fff81139a7   8     5     NaN          21.925          28.677   \n",
      "3  fff81139a7   7     5     NaN          29.603          21.925   \n",
      "4  fff81139a7   6     5     NaN          30.279          29.603   \n",
      "\n",
      "   target_shift12  target_shift13  target_shift14  target_shift15  ...  \\\n",
      "0          27.034          33.063          30.109          36.227  ...   \n",
      "1          33.691          27.034          33.063          30.109  ...   \n",
      "2          29.571          33.691          27.034          33.063  ...   \n",
      "3          28.677          29.571          33.691          27.034  ...   \n",
      "4          21.925          28.677          29.571          33.691  ...   \n",
      "\n",
      "   target_shift10_win50_mean  target_shift10_win50_max  \\\n",
      "0                   36.15146                    57.527   \n",
      "1                   36.35390                    57.527   \n",
      "2                   36.02730                    57.527   \n",
      "3                   35.87510                    57.527   \n",
      "4                   36.05080                    57.527   \n",
      "\n",
      "   target_shift10_win50_min  target_shift10_win50_sum  \\\n",
      "0                    22.631                  1807.573   \n",
      "1                    22.631                  1817.695   \n",
      "2                    22.631                  1801.365   \n",
      "3                    22.631                  1793.755   \n",
      "4                    22.631                  1802.540   \n",
      "\n",
      "   target_shift710win50_std  target_shift10_win70_mean  \\\n",
      "0                  6.716722                  35.678629   \n",
      "1                  6.812526                  36.159657   \n",
      "2                  6.929592                  36.068243   \n",
      "3                  7.063492                  35.954057   \n",
      "4                  7.086920                  35.977400   \n",
      "\n",
      "   target_shift10_win70_max  target_shift10_win70_min  \\\n",
      "0                    57.527                    10.779   \n",
      "1                    57.527                    22.631   \n",
      "2                    57.527                    22.631   \n",
      "3                    57.527                    22.631   \n",
      "4                    57.527                    22.631   \n",
      "\n",
      "   target_shift10_win70_sum  target_shift710win70_std  \n",
      "0                  2497.504                  7.157341  \n",
      "1                  2531.176                  6.566787  \n",
      "2                  2524.777                  6.669118  \n",
      "3                  2516.784                  6.770835  \n",
      "4                  2518.418                  6.786546  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "列名\n",
      "['dt', 'type', 'target_shift10', 'target_shift11', 'target_shift12', 'target_shift13', 'target_shift14', 'target_shift15', 'target_shift16', 'target_shift17', 'target_shift18', 'target_shift19', 'target_shift20', 'target_shift21', 'target_shift22', 'target_shift23', 'target_shift24', 'target_shift25', 'target_shift26', 'target_shift27', 'target_shift28', 'target_shift29', 'target_shift30', 'target_shift31', 'target_shift32', 'target_shift33', 'target_shift34', 'target_shift35', 'target_shift10_diff1', 'target_shift10_diff2', 'target_shift10_diff3', 'target_win15_mean', 'target_win15_max', 'target_win15_min', 'target_win15_std', 'target_win30_mean', 'target_win30_max', 'target_win30_min', 'target_win30_std', 'target_win50_mean', 'target_win50_max', 'target_win50_min', 'target_win50_std', 'target_win70_mean', 'target_win70_max', 'target_win70_min', 'target_win70_std', 'target_shift10_win7_mean', 'target_shift10_win7_max', 'target_shift10_win7_min', 'target_shift10_win7_sum', 'target_shift710win7_std', 'target_shift10_win14_mean', 'target_shift10_win14_max', 'target_shift10_win14_min', 'target_shift10_win14_sum', 'target_shift710win14_std', 'target_shift10_win28_mean', 'target_shift10_win28_max', 'target_shift10_win28_min', 'target_shift10_win28_sum', 'target_shift710win28_std', 'target_shift10_win35_mean', 'target_shift10_win35_max', 'target_shift10_win35_min', 'target_shift10_win35_sum', 'target_shift710win35_std', 'target_shift10_win50_mean', 'target_shift10_win50_max', 'target_shift10_win50_min', 'target_shift10_win50_sum', 'target_shift710win50_std', 'target_shift10_win70_mean', 'target_shift10_win70_max', 'target_shift10_win70_min', 'target_shift10_win70_sum', 'target_shift710win70_std']\n"
     ]
    }
   ],
   "source": [
    "# 进行数据切分\n",
    "# 选取了所有target值非空的记录，并重置索引，生成训练数据train。\n",
    "train = data[data.target.notnull()].reset_index(drop=True)\n",
    "print(\"train前5行数据\")\n",
    "print(train.head())\n",
    "\n",
    "# 选取了所有target值为空的记录，并重置索引，生成测试数据test。\n",
    "test = data[data.target.isnull()].reset_index(drop=True)\n",
    "print(\"test前5行数据\")\n",
    "print(test.head())\n",
    "\n",
    "# 确定输入特征\n",
    "# 使用列表推导式选取data中的所有列名，排除id和target列，生成用于模型训练的特征列表train_cols。\n",
    "train_cols = [f for f in data.columns if f not in ['id','target']]\n",
    "print(\"列名\")\n",
    "print(train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import log_evaluation, early_stopping\n",
    "# verbose_eval: 每200次迭代输出一次结果。\n",
    "# early_stopping_rounds: 如果在200次迭代后模型性能没有提升，则停止训练。\n",
    "callbacks = [log_evaluation(period=500), early_stopping(stopping_rounds=500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name, seed = 2024):\n",
    "    # '''\n",
    "    # clf: 传入的模型对象，可以是 LightGBM、XGBoost 或 CatBoost。\n",
    "    # train_x: 训练数据特征。\n",
    "    # train_y: 训练数据标签。\n",
    "    # test_x: 测试数据特征。\n",
    "    # clf_name: 使用模型的名称（'lgb'、'xgb' 或 'cat'）。\n",
    "    # seed: 随机种子，默认值为 2024。42，56，3407，4096\n",
    "    # '''\n",
    "\n",
    "    # 使用 KFold 进行 5 折交叉验证。KFold 将数据分成 5 份，每一份轮流作为验证集，其余作为训练集。\n",
    "    # 初始化 oof（out-of-fold predictions）数组，用于存储每折验证集的预测结果。\n",
    "    # 初始化 test_predict 数组，用于存储每折测试集的预测结果，最后取平均。\n",
    "    # 初始化 cv_scores 列表，用于存储每折的评估分数。\n",
    "    folds = 5\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(train_x.shape[0])\n",
    "    test_predict = np.zeros(test_x.shape[0])\n",
    "    cv_scores = []\n",
    "    \n",
    "    #  循环遍历每一折，分割训练集和验证集。\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "        \n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "        # 数据准备：\n",
    "        # train_matrix 和 valid_matrix 是 LightGBM 的 Dataset 对象，分别包含训练集和验证集数据及其标签。\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',    #提升类型，使用梯度提升决策树（GBDT）。\n",
    "                'objective': 'regression',  #目标函数，设为回归任务。\n",
    "                'metric': 'mae',            #评价指标，使用平均绝对误差（MAE）。\n",
    "                'min_child_weight': 6,      # 最小叶子节点样本权重和，用于防止过拟合。\n",
    "                'num_leaves': 2 ** 6,       #叶子节点数目，设置为 64。\n",
    "                'lambda_l2': 10,            #L2 正则化系数，防止过拟合。\n",
    "                'feature_fraction': 0.8,    #每次分裂时选择特征的比例。\n",
    "                'bagging_fraction': 0.8,    #每次迭代时数据采样的比例。\n",
    "                'bagging_freq': 4,          #Bagging 的频率。\n",
    "                'learning_rate': 0.1,       #学习率。\n",
    "                'seed': 2023,               #随机种子。\n",
    "                'nthread' : 16,             #使用的线程数。\n",
    "                'verbose' : -1,             #日志级别，表示不输出任何训练日志\n",
    "            }\n",
    "            # 模型训练：\n",
    "            # 调用 train 函数，使用指定参数进行训练，最大迭代次数为 1000。\n",
    "            # valid_sets 指定验证集，用于监控训练过程中的模型性能。\n",
    "            # callbacks 包含日志记录和早停回调函数。\n",
    "            model = clf.train(params, \n",
    "                              train_matrix, \n",
    "                              1000, \n",
    "                              valid_sets=[train_matrix, valid_matrix],\n",
    "                              categorical_feature=[], \n",
    "                              callbacks=callbacks\n",
    "                              )\n",
    "            \n",
    "            # 模型预测：\n",
    "            # predict 函数对验证集和测试集进行预测，使用最佳迭代次数（best_iteration）。\n",
    "            # 在训练过程中，LightGBM 会通过监控验证集的性能来决定最优的迭代次数。\n",
    "            # 这就是所谓的“早停”（early stopping）策略。\n",
    "            # 如果在一定数量的迭代（stopping_rounds）之后，验证集上的性能没有提升，训练就会停止，避免模型过拟合。\n",
    "            # model.best_iteration 是指在训练过程中，模型在验证集上表现最好的迭代次数。\n",
    "            # 使用最佳迭代次数对验证集 val_x 进行预测。\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            # 使用最佳迭代次数对测试集 test_x 进行预测。\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "        \n",
    "\n",
    "        if clf_name == \"xgb\":\n",
    "\n",
    "        # 数据准备：\n",
    "        # train_matrix、valid_matrix 和 test_matrix 是 XGBoost 的 DMatrix 对象，分别包含训练集、验证集和测试集数据及其标签。\n",
    "        # DMatrix 是 XGBoost 中的数据格式，用于高效地存储和处理数据。\n",
    "        # train_matrix、valid_matrix 和 test_matrix 分别是训练集、验证集和测试集的数据矩阵。\n",
    "        # trn_x 和 trn_y 是当前折的训练数据的特征和标签。\n",
    "        # val_x 和 val_y 是当前折的验证数据的特征和标签。\n",
    "        # test_x 是测试数据的特征。\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "\n",
    "        # watchlist 是一个元组列表，用于在训练过程中监控训练集和验证集的性能。\n",
    "        # train_matrix 和 valid_matrix 是之前定义的 XGBoost 数据矩阵。\n",
    "        # watchlist 中的元组格式为 (DMatrix, 名称)，其中名称用于标识输出日志中对应的数据集。           \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "\n",
    "            xgb_params = {\n",
    "              'booster': 'gbtree',              #提升器类型，使用基于树的模型。\n",
    "              'objective': 'reg:squarederror',  #目标函数，设为平方误差回归。\n",
    "              'eval_metric': 'mae',             #评价指标，使用平均绝对误差（MAE）。\n",
    "              'max_depth': 5,                   #树的最大深度。\n",
    "              'lambda': 10,                     #L2 正则化系数。\n",
    "              'subsample': 0.7,                 #每次迭代时数据采样的比例。\n",
    "              'colsample_bytree': 0.7,          # 每棵树使用的特征比例。\n",
    "              'colsample_bylevel': 0.7,         # 每一层使用的特征比例。\n",
    "              'eta': 0.1,                       #学习率。\n",
    "              'tree_method': 'hist',            #树的构建方法，使用直方图优化（hist）。\n",
    "              'seed': 520,                      #随机种子。\n",
    "              'nthread': 16                     # 使用的线程数。\n",
    "              }\n",
    "            \n",
    "            # 模型训练：\n",
    "            # 调用 train 函数，使用指定参数进行训练，最大迭代次数为 1000。\n",
    "            # evals 指定验证集，用于监控训练过程中的模型性能。\n",
    "            # verbose_eval 每 200 次迭代输出一次结果。\n",
    "            # early_stopping_rounds 指定早停轮数，如果在 100 次迭代后性能没有提升，则停止训练。\n",
    "            model = clf.train(xgb_params, \n",
    "                              train_matrix, \n",
    "                              num_boost_round=1000, \n",
    "                              evals=watchlist, \n",
    "                              verbose_eval=200, \n",
    "                              early_stopping_rounds=100\n",
    "                              )\n",
    "            \n",
    "            val_pred  = model.predict(valid_matrix)\n",
    "            test_pred = model.predict(test_matrix)\n",
    "        \n",
    "\n",
    "\n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.1,         #学习率。\n",
    "                      'depth': 5,                   #树的深度。\n",
    "                      'bootstrap_type':'Bernoulli', #采样类型，使用 Bernoulli 采样。\n",
    "                      'random_seed':2023,           #随机种子。\n",
    "                      'od_type': 'Iter',            #过拟合检测类型，使用迭代类型。\n",
    "                      'od_wait': 100,               #过拟合检测等待的轮数。\n",
    "                      'random_seed': 11, \n",
    "                      'allow_writing_files': False  #是否允许写入文件，设为 False 防止产生不必要的文件。\n",
    "                      }\n",
    "        \n",
    "            # 模型训练：\n",
    "            # 创建 CatBoost 模型对象，指定最大迭代次数为 1000。\n",
    "            # 调用 fit 函数，进行模型训练，设置验证集用于监控模型性能。\n",
    "            # metric_period 每 200 次迭代输出一次结果。\n",
    "            # use_best_model 启用选择最佳模型。\n",
    "            # cat_features 指定分类特征列表，这里为空。\n",
    "            # verbose 控制输出日志的详细程度。\n",
    "            model = clf(iterations=1000, **params)\n",
    "            model.fit(trn_x, trn_y, \n",
    "                      eval_set=(val_x, val_y),\n",
    "                      metric_period=200,\n",
    "                      use_best_model=True, \n",
    "                      cat_features=[],\n",
    "                      verbose=1)\n",
    "            \n",
    "            # 模型预测：\n",
    "            # predict 函数对验证集和测试集进行预测。\n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "        \n",
    "        # 将验证集预测结果存储到 oof 中。\n",
    "        # 累加每折的测试集预测结果。\n",
    "        # 计算验证集的平均绝对误差（MAE），并存储到 cv_scores 中。\n",
    "        oof[valid_index] = val_pred\n",
    "        test_predict += test_pred / kf.n_splits\n",
    "        \n",
    "        score = mean_absolute_error(val_y, val_pred)\n",
    "        cv_scores.append(score)\n",
    "        print(cv_scores)\n",
    "    #返回 out-of-fold 预测结果和测试集预测结果。\n",
    "    return oof, test_predict\n",
    "\n",
    "# 选择lightgbm模型\n",
    "lgb_oof, lgb_test = cv_model(lgb, train[train_cols], train['target'], test[train_cols], 'lgb')\n",
    "# 选择xgboost模型\n",
    "xgb_oof, xgb_test = cv_model(xgb, train[train_cols], train['target'], test[train_cols], 'xgb')\n",
    "# 选择catboost模型\n",
    "cat_oof, cat_test = cv_model(CatBoostRegressor, train[train_cols], train['target'], test[train_cols], 'cat')\n",
    "\n",
    "# 进行取平均融合\n",
    "final_test = (lgb_test + xgb_test + cat_test) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's l1: 6.55896\tvalid_1's l1: 6.82078\n",
      "[1000]\ttraining's l1: 6.31344\tvalid_1's l1: 6.72825\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\ttraining's l1: 6.31392\tvalid_1's l1: 6.72819\n",
      "[6.7281933623999235]\n",
      "************************************ 2 ************************************\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's l1: 6.56275\tvalid_1's l1: 6.81872\n",
      "[1000]\ttraining's l1: 6.31979\tvalid_1's l1: 6.72998\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l1: 6.31979\tvalid_1's l1: 6.72998\n",
      "[6.7281933623999235, 6.729978403365649]\n",
      "************************************ 3 ************************************\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's l1: 6.5647\tvalid_1's l1: 6.80756\n",
      "[1000]\ttraining's l1: 6.31943\tvalid_1's l1: 6.71652\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l1: 6.31943\tvalid_1's l1: 6.71652\n",
      "[6.7281933623999235, 6.729978403365649, 6.716520623008887]\n",
      "************************************ 4 ************************************\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's l1: 6.55748\tvalid_1's l1: 6.8088\n",
      "[1000]\ttraining's l1: 6.31975\tvalid_1's l1: 6.72664\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l1: 6.31975\tvalid_1's l1: 6.72664\n",
      "[6.7281933623999235, 6.729978403365649, 6.716520623008887, 6.726638870039725]\n",
      "************************************ 5 ************************************\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's l1: 6.56345\tvalid_1's l1: 6.81917\n",
      "[1000]\ttraining's l1: 6.31947\tvalid_1's l1: 6.73287\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l1: 6.31947\tvalid_1's l1: 6.73287\n",
      "[6.7281933623999235, 6.729978403365649, 6.716520623008887, 6.726638870039725, 6.732871178025917]\n",
      "************************************ 1 ************************************\n",
      "[0]\ttrain-mae:20.00049\teval-mae:19.99792\n",
      "[200]\ttrain-mae:7.06136\teval-mae:7.11722\n",
      "[400]\ttrain-mae:6.93038\teval-mae:7.02988\n",
      "[600]\ttrain-mae:6.84186\teval-mae:6.97874\n",
      "[800]\ttrain-mae:6.77587\teval-mae:6.94652\n",
      "[999]\ttrain-mae:6.71787\teval-mae:6.91878\n",
      "[6.918781668316242]\n",
      "************************************ 2 ************************************\n",
      "[0]\ttrain-mae:20.01594\teval-mae:20.05564\n",
      "[200]\ttrain-mae:7.05585\teval-mae:7.10945\n",
      "[400]\ttrain-mae:6.92765\teval-mae:7.02661\n",
      "[600]\ttrain-mae:6.84443\teval-mae:6.98173\n",
      "[800]\ttrain-mae:6.77825\teval-mae:6.94959\n",
      "[999]\ttrain-mae:6.72470\teval-mae:6.92670\n",
      "[6.918781668316242, 6.926704442588898]\n",
      "************************************ 3 ************************************\n",
      "[0]\ttrain-mae:20.01124\teval-mae:19.98984\n",
      "[200]\ttrain-mae:7.05427\teval-mae:7.09421\n",
      "[400]\ttrain-mae:6.93123\teval-mae:7.01306\n",
      "[600]\ttrain-mae:6.84737\teval-mae:6.96710\n",
      "[800]\ttrain-mae:6.78165\teval-mae:6.93380\n",
      "[999]\ttrain-mae:6.72817\teval-mae:6.91220\n",
      "[6.918781668316242, 6.926704442588898, 6.912195580149546]\n",
      "************************************ 4 ************************************\n",
      "[0]\ttrain-mae:20.00662\teval-mae:19.99050\n",
      "[200]\ttrain-mae:7.05877\teval-mae:7.11140\n",
      "[400]\ttrain-mae:6.92925\teval-mae:7.02358\n",
      "[600]\ttrain-mae:6.85023\teval-mae:6.98090\n",
      "[800]\ttrain-mae:6.78330\teval-mae:6.94595\n",
      "[999]\ttrain-mae:6.72461\teval-mae:6.91791\n",
      "[6.918781668316242, 6.926704442588898, 6.912195580149546, 6.917911069666927]\n",
      "************************************ 5 ************************************\n",
      "[0]\ttrain-mae:20.00286\teval-mae:20.00528\n",
      "[200]\ttrain-mae:7.05238\teval-mae:7.11558\n",
      "[400]\ttrain-mae:6.92444\teval-mae:7.02920\n",
      "[600]\ttrain-mae:6.84009\teval-mae:6.98235\n",
      "[800]\ttrain-mae:6.78048\teval-mae:6.95654\n",
      "[999]\ttrain-mae:6.72565\teval-mae:6.93211\n",
      "[6.918781668316242, 6.926704442588898, 6.912195580149546, 6.917911069666927, 6.932111318270082]\n",
      "************************************ 1 ************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 46.3013554\ttest: 46.4794383\tbest: 46.4794383 (0)\ttotal: 352ms\tremaining: 5m 52s\n",
      "200:\tlearn: 14.0004515\ttest: 14.0463593\tbest: 14.0463593 (200)\ttotal: 41.6s\tremaining: 2m 45s\n",
      "400:\tlearn: 13.4367973\ttest: 13.6020852\tbest: 13.6020852 (400)\ttotal: 1m 24s\tremaining: 2m 5s\n",
      "600:\tlearn: 13.1351201\ttest: 13.3900200\tbest: 13.3900200 (600)\ttotal: 2m 6s\tremaining: 1m 23s\n",
      "800:\tlearn: 12.9029584\ttest: 13.2496119\tbest: 13.2496119 (800)\ttotal: 2m 51s\tremaining: 42.7s\n",
      "999:\tlearn: 12.7313688\ttest: 13.1598318\tbest: 13.1598318 (999)\ttotal: 3m 34s\tremaining: 0us\n",
      "\n",
      "bestTest = 13.15983176\n",
      "bestIteration = 999\n",
      "\n",
      "[7.064867606955804]\n",
      "************************************ 2 ************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 46.3653360\ttest: 46.1890329\tbest: 46.1890329 (0)\ttotal: 204ms\tremaining: 3m 24s\n",
      "200:\tlearn: 13.9958084\ttest: 14.0944545\tbest: 14.0944545 (200)\ttotal: 41.9s\tremaining: 2m 46s\n",
      "400:\tlearn: 13.4364200\ttest: 13.6327754\tbest: 13.6327754 (400)\ttotal: 1m 24s\tremaining: 2m 6s\n",
      "600:\tlearn: 13.1105704\ttest: 13.4130411\tbest: 13.4129390 (597)\ttotal: 2m 7s\tremaining: 1m 24s\n",
      "800:\tlearn: 12.8913936\ttest: 13.2810127\tbest: 13.2810127 (800)\ttotal: 2m 47s\tremaining: 41.6s\n",
      "999:\tlearn: 12.7170494\ttest: 13.1842582\tbest: 13.1842582 (999)\ttotal: 3m 27s\tremaining: 0us\n",
      "\n",
      "bestTest = 13.18425819\n",
      "bestIteration = 999\n",
      "\n",
      "[7.064867606955804, 7.064302682173274]\n",
      "************************************ 3 ************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 46.3143483\ttest: 46.4145671\tbest: 46.4145671 (0)\ttotal: 206ms\tremaining: 3m 25s\n",
      "200:\tlearn: 14.0304791\ttest: 14.0064205\tbest: 14.0064205 (200)\ttotal: 43.9s\tremaining: 2m 54s\n",
      "400:\tlearn: 13.4455836\ttest: 13.5200526\tbest: 13.5200526 (400)\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "600:\tlearn: 13.1652347\ttest: 13.3297732\tbest: 13.3297732 (600)\ttotal: 2m 8s\tremaining: 1m 25s\n",
      "800:\tlearn: 12.9478597\ttest: 13.1898428\tbest: 13.1898428 (800)\ttotal: 2m 46s\tremaining: 41.4s\n",
      "999:\tlearn: 12.7713201\ttest: 13.0889703\tbest: 13.0889703 (999)\ttotal: 3m 25s\tremaining: 0us\n",
      "\n",
      "bestTest = 13.08897033\n",
      "bestIteration = 999\n",
      "\n",
      "[7.064867606955804, 7.064302682173274, 7.055804377966254]\n",
      "************************************ 4 ************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 46.3708450\ttest: 46.1735689\tbest: 46.1735689 (0)\ttotal: 208ms\tremaining: 3m 28s\n",
      "200:\tlearn: 14.0154822\ttest: 14.0881818\tbest: 14.0881818 (200)\ttotal: 38.9s\tremaining: 2m 34s\n",
      "400:\tlearn: 13.4149642\ttest: 13.6015942\tbest: 13.6015942 (400)\ttotal: 1m 17s\tremaining: 1m 55s\n",
      "600:\tlearn: 13.1134896\ttest: 13.3881497\tbest: 13.3881497 (600)\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "800:\tlearn: 12.9126392\ttest: 13.2645631\tbest: 13.2645631 (800)\ttotal: 2m 35s\tremaining: 38.7s\n",
      "999:\tlearn: 12.7359276\ttest: 13.1501330\tbest: 13.1501330 (999)\ttotal: 3m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 13.15013298\n",
      "bestIteration = 999\n",
      "\n",
      "[7.064867606955804, 7.064302682173274, 7.055804377966254, 7.065730191652328]\n",
      "************************************ 5 ************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 46.3081884\ttest: 46.4182915\tbest: 46.4182915 (0)\ttotal: 183ms\tremaining: 3m 2s\n",
      "200:\tlearn: 13.8979556\ttest: 14.2178675\tbest: 14.2178675 (200)\ttotal: 38.5s\tremaining: 2m 33s\n",
      "400:\tlearn: 13.3531667\ttest: 13.7593956\tbest: 13.7593956 (400)\ttotal: 1m 17s\tremaining: 1m 55s\n",
      "600:\tlearn: 13.0578200\ttest: 13.5456391\tbest: 13.5456391 (600)\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "800:\tlearn: 12.8526857\ttest: 13.4182427\tbest: 13.4182427 (800)\ttotal: 2m 35s\tremaining: 38.7s\n",
      "999:\tlearn: 12.6784698\ttest: 13.3173631\tbest: 13.3173631 (999)\ttotal: 3m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 13.31736306\n",
      "bestIteration = 999\n",
      "\n",
      "[7.064867606955804, 7.064302682173274, 7.055804377966254, 7.065730191652328, 7.063521892130133]\n"
     ]
    }
   ],
   "source": [
    "# 函数参数说明\n",
    "# clf: 传入的模型对象，可以是 LightGBM、XGBoost 或 CatBoost。\n",
    "# train_x: 训练数据的特征。\n",
    "# train_y: 训练数据的标签。\n",
    "# test_x: 测试数据的特征。\n",
    "# clf_name: 使用模型的名称（'lgb'、'xgb' 或 'cat'）。\n",
    "# seed: 随机种子，默认值为 2024。\n",
    "# param_search: 参数搜索的范围，如果需要进行超参数优化，可以传入相应的参数范围。default=None\n",
    "# search_type: 搜索类型，默认为 'grid'（网格搜索）。\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, seed=2024, param_search=None, search_type='grid'):\n",
    "\n",
    "# KFold交叉验证\n",
    "# 定义 5 折交叉验证，使用 KFold 将数据分成 5 份，每一份轮流作为验证集，其余作为训练集。\n",
    "# shuffle=True 确保数据在分割之前被随机打乱，random_state=seed 用于保证结果可复现。\n",
    "# 初始化 oof（out-of-fold predictions）数组，用于存储每折验证集的预测结果。\n",
    "# 初始化 test_predict 数组，用于存储每折测试集的预测结果，最后取平均。\n",
    "# 初始化 cv_scores 列表，用于存储每折的评估分数。\n",
    "    folds = 5\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    oof = np.zeros(train_x.shape[0])\n",
    "    test_predict = np.zeros(test_x.shape[0])\n",
    "    cv_scores = []\n",
    "    \n",
    "# 循环遍历每一折的训练和验证集索引，通过 kf.split(train_x, train_y) 获取当前折的训练集和验证集索引。\n",
    "# 使用 train_index 和 valid_index 分割训练数据和验证数据。\n",
    "# trn_x 和 trn_y 分别是当前折的训练集特征和标签。\n",
    "# val_x 和 val_y 分别是当前折的验证集特征和标签。  \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        # 打印当前折的编号，方便跟踪训练过程。  \n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "        \n",
    "        if clf_name == \"lgb\":\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',    #提升类型，使用梯度提升决策树（GBDT）。\n",
    "                'objective': 'regression',  #目标函数，设为回归任务。\n",
    "                'metric': 'mae',            #评价指标，使用平均绝对误差（MAE）。\n",
    "                'min_child_weight': 6,      # 最小叶子节点样本权重和，用于防止过拟合。\n",
    "                'num_leaves': 2 ** 6,       #叶子节点数目，设置为 64。\n",
    "                'lambda_l2': 10,            #L2 正则化系数，防止过拟合。\n",
    "                'feature_fraction': 0.8,    #每次分裂时选择特征的比例。\n",
    "                'bagging_fraction': 0.8,    #每次迭代时数据采样的比例。\n",
    "                'bagging_freq': 4,          #Bagging 的频率。\n",
    "                'learning_rate': 0.1,       #学习率。\n",
    "                'seed': 42,               #随机种子。\n",
    "                'nthread' : 16,             #使用的线程数。\n",
    "                'verbose' : -1,             #日志级别，表示不输出任何训练日志\n",
    "            }\n",
    "\n",
    "            # 看是否传入参数parm_search(!=None)\n",
    "            if param_search:\n",
    "\n",
    "                # 如果 search_type 为 'grid'，则使用 GridSearchCV 进行网格搜索。\n",
    "                # clf.LGBMRegressor(**params)：创建一个 LightGBM 回归模型，使用传入的 params 参数初始化。\n",
    "                # param_search：定义了需要搜索的参数范围。\n",
    "                # scoring='neg_mean_absolute_error'：使用负平均绝对误差（MAE）作为评分标准。\n",
    "                # cv=3：使用 3 折交叉验证。\n",
    "                # # verbose=1：输出详细日志信息。\n",
    "                if search_type == 'grid':\n",
    "                    search = GridSearchCV(estimator=lgb.LGBMRegressor(**params), param_grid=param_search, scoring='neg_mean_absolute_error', cv=3, verbose=1)\n",
    "                    \n",
    "                # 如果 search_type 为 'random'，则使用 RandomizedSearchCV 进行随机搜索。\n",
    "                # n_iter=50：指定随机搜索的迭代次数为 50。\n",
    "                # random_state=seed：设置随机种子，确保结果可复现。                \n",
    "                elif search_type == 'random':\n",
    "                    search = RandomizedSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, verbose=1, random_state=seed)\n",
    "                    \n",
    "                # 如果 search_type 为其他值，默认使用 BayesSearchCV 进行贝叶斯优化搜索。\n",
    "                # n_iter=50：指定搜索的迭代次数为 50。                \n",
    "                else:\n",
    "                    search = BayesSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, random_state=seed)\n",
    "                    \n",
    "                # 使用搜索得到的最佳参数更新 params。\n",
    "                # search.best_params_ 包含了搜索过程中找到的最优参数。\n",
    "                search.fit(trn_x, trn_y)\n",
    "                params.update(search.best_params_)\n",
    "            \n",
    "            #train_matrix 和 valid_matrix 是 LightGBM 的 Dataset 对象，分别包含训练集和验证集数据及其标签。\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            # 模型训练：\n",
    "            # 调用 train 函数，使用指定参数进行训练，最大迭代次数为 1000。\n",
    "            # valid_sets 指定验证集，用于监控训练过程中的模型性能。\n",
    "            # callbacks 包含日志记录和早停回调函数。\n",
    "            model = clf.train(params, \n",
    "                              train_matrix, \n",
    "                              1000, \n",
    "                              valid_sets=[train_matrix, valid_matrix],\n",
    "                              categorical_feature=[], \n",
    "                              callbacks=callbacks\n",
    "                              )\n",
    "            \n",
    "            # 模型预测：\n",
    "            # predict 函数对验证集和测试集进行预测，使用最佳迭代次数（best_iteration）。\n",
    "            # 在训练过程中，LightGBM 会通过监控验证集的性能来决定最优的迭代次数。\n",
    "            # 这就是所谓的“早停”（early stopping）策略。\n",
    "            # 如果在一定数量的迭代（stopping_rounds）之后，验证集上的性能没有提升，训练就会停止，避免模型过拟合。\n",
    "            # model.best_iteration 是指在训练过程中，模型在验证集上表现最好的迭代次数。\n",
    "            # 使用最佳迭代次数对验证集 val_x 进行预测。\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            # 使用最佳迭代次数对测试集 test_x 进行预测。\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "        \n",
    "        if clf_name == \"xgb\":\n",
    "            \n",
    "            xgb_params = {\n",
    "              'booster': 'gbtree',              #提升器类型，使用基于树的模型。\n",
    "              'objective': 'reg:squarederror',  #目标函数，设为平方误差回归。\n",
    "              'eval_metric': 'mae',             #评价指标，使用平均绝对误差（MAE）。\n",
    "              'max_depth': 5,                   #树的最大深度。\n",
    "              'lambda': 10,                     #L2 正则化系数。\n",
    "              'subsample': 0.7,                 #每次迭代时数据采样的比例。\n",
    "              'colsample_bytree': 0.7,          # 每棵树使用的特征比例。\n",
    "              'colsample_bylevel': 0.7,         # 每一层使用的特征比例。\n",
    "              'eta': 0.1,                       #学习率。\n",
    "              'tree_method': 'hist',            #树的构建方法，使用直方图优化（hist）。\n",
    "              'seed': 42,                      #随机种子。\n",
    "              'nthread': 16                     # 使用的线程数。\n",
    "              }\n",
    "            \n",
    "            if param_search:\n",
    "\n",
    "                # 如果 search_type 为 'grid'，则使用 GridSearchCV 进行网格搜索。\n",
    "                # clf.LGBMRegressor(**params)：创建一个 LightGBM 回归模型，使用传入的 params 参数初始化。\n",
    "                # param_search：定义了需要搜索的参数范围。\n",
    "                # scoring='neg_mean_absolute_error'：使用负平均绝对误差（MAE）作为评分标准。\n",
    "                # cv=3：使用 3 折交叉验证。\n",
    "                # # verbose=1：输出详细日志信息。\n",
    "                if search_type == 'grid':\n",
    "                    search = GridSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, verbose=1)\n",
    "                    \n",
    "                # 如果 search_type 为 'random'，则使用 RandomizedSearchCV 进行随机搜索。\n",
    "                # n_iter=50：指定随机搜索的迭代次数为 50。\n",
    "                # random_state=seed：设置随机种子，确保结果可复现。                \n",
    "                elif search_type == 'random':\n",
    "                    search = RandomizedSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, verbose=1, random_state=seed)\n",
    "                    \n",
    "                # 如果 search_type 为其他值，默认使用 BayesSearchCV 进行贝叶斯优化搜索。\n",
    "                # n_iter=50：指定搜索的迭代次数为 50。                \n",
    "                else:\n",
    "                    search = BayesSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, random_state=seed)\n",
    "                    \n",
    "                # 使用搜索得到的最佳参数更新 params。\n",
    "                # search.best_params_ 包含了搜索过程中找到的最优参数。\n",
    "                search.fit(trn_x, trn_y)\n",
    "                params.update(search.best_params_)\n",
    "            \n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(xgb_params, train_matrix, num_boost_round=1000, evals=watchlist, verbose_eval=200, early_stopping_rounds=100)\n",
    "            val_pred  = model.predict(valid_matrix)\n",
    "            test_pred = model.predict(test_matrix)\n",
    "            \n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.1, 'depth': 5, 'bootstrap_type':'Bernoulli','random_seed':42,\n",
    "                      'od_type': 'Iter', 'od_wait': 100, 'random_seed': 11, 'allow_writing_files': False}\n",
    "\n",
    "            if param_search:\n",
    "\n",
    "                # 如果 search_type 为 'grid'，则使用 GridSearchCV 进行网格搜索。\n",
    "                # clf.LGBMRegressor(**params)：创建一个 LightGBM 回归模型，使用传入的 params 参数初始化。\n",
    "                # param_search：定义了需要搜索的参数范围。\n",
    "                # scoring='neg_mean_absolute_error'：使用负平均绝对误差（MAE）作为评分标准。\n",
    "                # cv=3：使用 3 折交叉验证。\n",
    "                # # verbose=1：输出详细日志信息。\n",
    "                if search_type == 'grid':\n",
    "                    search = GridSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, verbose=1)\n",
    "                    \n",
    "                # 如果 search_type 为 'random'，则使用 RandomizedSearchCV 进行随机搜索。\n",
    "                # n_iter=50：指定随机搜索的迭代次数为 50。\n",
    "                # random_state=seed：设置随机种子，确保结果可复现。                \n",
    "                elif search_type == 'random':\n",
    "                    search = RandomizedSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, verbose=1, random_state=seed)\n",
    "                    \n",
    "                # 如果 search_type 为其他值，默认使用 BayesSearchCV 进行贝叶斯优化搜索。\n",
    "                # n_iter=50：指定搜索的迭代次数为 50。                \n",
    "                else:\n",
    "                    search = BayesSearchCV(lgb.LGBMRegressor(**params), param_search, scoring='neg_mean_absolute_error', cv=3, n_iter=50, random_state=seed)\n",
    "                    \n",
    "                # 使用搜索得到的最佳参数更新 params。\n",
    "                # search.best_params_ 包含了搜索过程中找到的最优参数。\n",
    "                search.fit(trn_x, trn_y)\n",
    "                params.update(search.best_params_)\n",
    "\n",
    "            model = clf(iterations=1000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      metric_period=200,\n",
    "                      use_best_model=True, \n",
    "                      verbose=1)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "        \n",
    "        oof[valid_index] = val_pred\n",
    "        test_predict += test_pred / kf.n_splits\n",
    "        \n",
    "        score = mean_absolute_error(val_y, val_pred)\n",
    "        cv_scores.append(score)\n",
    "        print(cv_scores)\n",
    "        \n",
    "    return oof, test_predict\n",
    "\n",
    "# 参数搜索空间示例\n",
    "param_search = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# 选择lightgbm模型并进行参数优化\n",
    "lgb_oof, lgb_test = cv_model(lgb, train[train_cols], train['target'], test[train_cols], 'lgb', param_search, search_type='grid')\n",
    "\n",
    "# 参数搜索空间示例\n",
    "param_search = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# 选择xgboost模型并进行参数优化\n",
    "xgb_oof, xgb_test = cv_model(xgb, train[train_cols], train['target'], test[train_cols], 'xgb', param_search, search_type='random')\n",
    "\n",
    "# 参数搜索空间示例\n",
    "param_search = {\n",
    "    'depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# 选择catboost模型并进行参数优化\n",
    "cat_oof, cat_test = cv_model(CatBoostRegressor, train[train_cols], train['target'], test[train_cols], 'cat', param_search, search_type='bayes')\n",
    "\n",
    "# 进行取平均融合\n",
    "final_test1 = (lgb_test + xgb_test + cat_test) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果文件到本地\n",
    "test['target'] = final_test1\n",
    "test[['id','dt','target']].to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "1/5 6.725996253671618\n",
      "fold n°2\n",
      "2/5 6.726375409966235\n",
      "fold n°3\n",
      "3/5 6.71257118891519\n",
      "fold n°4\n",
      "4/5 6.71954283665174\n",
      "fold n°5\n",
      "5/5 6.7293882506098965\n",
      "fold n°6\n",
      "6/5 6.71210481424597\n",
      "fold n°7\n",
      "7/5 6.7221608755847235\n",
      "fold n°8\n",
      "8/5 6.714667900893997\n",
      "fold n°9\n",
      "9/5 6.733736214937944\n",
      "fold n°10\n",
      "10/5 6.73100587455158\n",
      "mean:  6.722754962002888\n"
     ]
    }
   ],
   "source": [
    "#stacking融合\n",
    "# 参数：\n",
    "# oof_1, oof_2, oof_3: 模型1、模型2、模型3的 out-of-fold (OOF) 预测结果，\n",
    "# 分别对应 LightGBM、XGBoost 和 CatBoost 模型的 OOF 预测。\n",
    "# predictions_1, predictions_2, predictions_3: 模型1、模型2、模型3对测试集的预测结果，\n",
    "# 分别对应 LightGBM、XGBoost 和 CatBoost 模型的测试集预测。\n",
    "# y: 训练数据的实际标签。\n",
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y):\n",
    "\n",
    "    # '''\n",
    "    # 输入的oof_1, oof_2, oof_3可以对应lgb_oof，xgb_oof，cat_oof\n",
    "    # predictions_1, predictions_2, predictions_3对应lgb_test，xgb_test，cat_test\n",
    "    # '''\n",
    "\n",
    "    # 1.创建训练和测试数据的堆叠：\n",
    "    # 将 oof_1, oof_2, oof_3 拼接成训练数据的堆叠矩阵 train_stack。\n",
    "    train_stack = pd.concat([oof_1, oof_2, oof_3], axis=1)\n",
    "    # 将 predictions_1, predictions_2, predictions_3 拼接成测试数据的堆叠矩阵 test_stack。\n",
    "    test_stack = pd.concat([predictions_1, predictions_2, predictions_3], axis=1)\n",
    "    \n",
    "    # 2.初始化存储变量：\n",
    "    # oof: 存储堆叠模型的 OOF 预测结果。shape[0]返回行数\n",
    "    # predictions: 存储堆叠模型的测试集预测结果。\n",
    "    # scores: 存储每一折交叉验证的 MAE 分数。\n",
    "    oof = np.zeros((train_stack.shape[0],))\n",
    "    predictions = np.zeros((test_stack.shape[0],))\n",
    "    scores = []\n",
    "    \n",
    "    # 3.定义交叉验证：\n",
    "    # 使用 RepeatedKFold 进行交叉验证，设置 n_splits=5 和 n_repeats=2，即5折交叉验证重复2次。\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "    \n",
    "    # 4.交叉验证训练和预测：\n",
    "    # 循环遍历每一折交叉验证，将数据分为训练集和验证集。\n",
    "    # 初始化一个 Ridge 回归模型进行训练。\n",
    "    # 使用训练好的模型对验证集和测试集进行预测。\n",
    "    # 计算每一折的 MAE 分数并存储。\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, train_stack)): \n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data, trn_y = train_stack.loc[trn_idx], y[trn_idx]\n",
    "        val_data, val_y = train_stack.loc[val_idx], y[val_idx]\n",
    "        \n",
    "        clf = Ridge(random_state=42)\n",
    "        clf.fit(trn_data, trn_y)\n",
    "\n",
    "        oof[val_idx] = clf.predict(val_data)\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "        \n",
    "        score_single = mean_absolute_error(val_y, oof[val_idx])\n",
    "        scores.append(score_single)\n",
    "        print(f'{fold_+1}/{5}', score_single)\n",
    "    print('mean: ',np.mean(scores))\n",
    "   \n",
    "    # 5.返回结果：\n",
    "    # oof: 堆叠模型的 OOF 预测结果。\n",
    "    # predictions: 堆叠模型的测试集预测结果。\n",
    "    return oof, predictions\n",
    "\n",
    "# 6.调用 stack_model 函数：\n",
    "# 传入 LightGBM、XGBoost 和 CatBoost 模型的 OOF 预测和测试集预测，以及训练数据的标签 train['target']。\n",
    "# 获取堆叠模型的 OOF 预测结果 stack_oof 和测试集预测结果 stack_pred。\n",
    "stack_oof, stack_pred = stack_model(pd.DataFrame(lgb_oof), pd.DataFrame(xgb_oof), pd.DataFrame(cat_oof), \n",
    "                                    pd.DataFrame(lgb_test), pd.DataFrame(xgb_test), pd.DataFrame(cat_test), train['target'])\n",
    "# 7.输出\n",
    "# 将堆叠模型的测试集预测结果 stack_pred 赋值给 final_test。\n",
    "final_test=stack_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果文件到本地\n",
    "test['target'] = final_test\n",
    "test[['id','dt','target']].to_csv('submit3.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
